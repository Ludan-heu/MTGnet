{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch\n",
    "import random\n",
    "#import matplotlib.pyplot as plt\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "import pandas as pd\n",
    "#网络结构\n",
    "class TGCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TGCN, self).__init__()\n",
    "        self.hidden_size = 64\n",
    "        self.reset_parameters()\n",
    "        self.stations = 35\n",
    "        self.features = 29\n",
    "        self.forecast = 6\n",
    "        self.historical = 12\n",
    "        self.tem = 3\n",
    "        self.wf = nn.Parameter(torch.FloatTensor(self.hidden_size, hidden_size))\n",
    "        self.bf = nn.Parameter(torch.FloatTensor(self.hidden_size))\n",
    "        self.a = nn.Parameter(torch.FloatTensor(1))\n",
    "        self.encoder = nn.Parameter(torch.FloatTensor(self.features, self.hidden_size))\n",
    "        self.bencoder = nn.Parameter(torch.FloatTensor(self.hidden_size))\n",
    "        self.decoder = nn.Parameter(torch.FloatTensor(self.hidden_size, self.hidden_size))\n",
    "        self.bdecoder = nn.Parameter(torch.FloatTensor(self.hidden_size))\n",
    "        self.Z1 = nn.Parameter(torch.FloatTensor(self.hidden_size*2, self.hidden_size))\n",
    "        self.Ws = nn.Parameter(torch.FloatTensor(self.hidden_size, self.hidden_size))\n",
    "        self.Wp = nn.Parameter(torch.FloatTensor(1, self.hidden_size))\n",
    "        self.Wgcn = nn.Parameter(torch.FloatTensor(self.hidden_size, self.hidden_size))\n",
    "        self.Wgcn1 = nn.Parameter(torch.FloatTensor(self.hidden_size, self.hidden_size))\n",
    "        self.Wgcn2 = nn.Parameter(torch.FloatTensor(self.hidden_size, self.hidden_size))\n",
    "        self.Wgcn3 = nn.Parameter(torch.FloatTensor(self.hidden_size, self.hidden_size))\n",
    "        self.Wgcn4 = nn.Parameter(torch.FloatTensor(self.hidden_size, self.hidden_size))\n",
    "        self.con1 = nn.Conv2d(self.hidden_size, self.hidden_size, (self.tem, 1), padding=(1, 0))\n",
    "        self.con2 = nn.Conv2d(self.hidden_size, self.hidden_size, (self.historical, 1))\n",
    "        self.averagecon = nn.Conv2d(self.hidden_size, self.hidden_size, (self.historical, 1))\n",
    "        self.wshort = nn.Parameter(torch.FloatTensor(self.stations, self.stations))\n",
    "        self.wc = nn.Parameter(torch.FloatTensor(self.hidden_size*2, self.hidden_size))\n",
    "        self.dcon1 = nn.ConvTranspose2d(self.hidden_size, self.hidden_size, (1, self.stations))\n",
    "        self.wfusion = nn.Parameter(torch.FloatTensor(self.hidden_size*2, self.hidden_size))\n",
    "        self.wout = nn.Parameter(torch.FloatTensor(self.hidden_size, 1))\n",
    "        self.avgwout = nn.Parameter(torch.FloatTensor(self.hidden_size, 1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for weight in self.parameters():\n",
    "            init.uniform_(weight, -0.1, 0.1)\n",
    "\n",
    "    #输入数据格式转化\n",
    "    def input_transform(self, x):\n",
    "        local_inputs, labels = x\n",
    "        local_inputs = local_inputs.permute(1, 0, 2, 3) #(12h, batch, features, stations)\n",
    "        labels = labels.permute(1, 0, 2, 3) #(6h, batch, 1features, stations)\n",
    "        n_input_encoder = local_inputs.data.size(2) #28features\n",
    "        batch_size = local_inputs.data.size(1)\n",
    "        _local_inputs = local_inputs.contiguous().view(-1, n_input_encoder, self.stations) #(12*batch,features,stations)\n",
    "        _local_inputs = torch.split(_local_inputs, batch_size, 0) #12个数组 每个数组（batch，stations，features）\n",
    "        encoder_inputs = _local_inputs\n",
    "        _labels = labels.contiguous().view(-1, self.stations, 1)\n",
    "        _labels = torch.split(_labels, batch_size, 0) #6个数组 （batch, 1, stations）\n",
    "        _lastinp = local_inputs[11:12, :, 0:1, :]  # 1, 256, 1, 35\n",
    "        _lastinp = _lastinp.contiguous().view(-1, 1, self.stations)\n",
    "        _lastinp = torch.split(_lastinp, batch_size, 0)\n",
    "        decoder_inputs = list(_lastinp) + list(_labels[:-1])  # 6*(256, 1, 35)\n",
    "        return encoder_inputs, _labels, decoder_inputs\n",
    "\n",
    "    #第一层lstm\n",
    "    def SpatialGCN(self, encoder_inputs, decoder_inputs, Ws, teather_ratio):\n",
    "        Inputs = encoder_inputs #12 (batch, stations, m)\n",
    "        batch_size = Inputs[0].data.size(0)\n",
    "        stations = Inputs[0].data.size(2)\n",
    "        temporalM = torch.FloatTensor(batch_size, self.historical, stations, self.hidden_size)\n",
    "        memory_cell = torch.rand(stations, self.hidden_size).to(device)\n",
    "        index = 0\n",
    "        for flinput in Inputs: #n个时间步\n",
    "            dec = torch.FloatTensor(batch_size, stations, self.hidden_size).to(device)\n",
    "            for s in range(flinput.data.size(2)):\n",
    "                en_out = torch.matmul(flinput[:, :, s].float(),self.encoder) + self.bencoder\n",
    "                de_out = torch.matmul(en_out, self.decoder) + self.bdecoder\n",
    "                dec[:, s:s+1, :] = de_out.reshape(batch_size, 1, self.hidden_size)\n",
    "                #dec[:, s:s+1, :] =  flinput[:, :, s:s+1].float().permute(0, 2, 1)\n",
    "            input_ = torch.cat([dec.to(device), memory_cell.reshape(1, stations, self.hidden_size).repeat(batch_size,1,1)], 2)  \n",
    "            memory_cell = self.a * memory_cell +(1-self.a) * torch.matmul(torch.mean(dec,0), self.wf) + self.bf\n",
    "            input_ = torch.matmul(input_, self.Z1)\n",
    "            #input_ = dec.to(device)\n",
    "            top = torch.exp(torch.sigmoid(torch.matmul(torch.matmul(input_, Ws), input_.permute(0, 2, 1))))\n",
    "            bottom = torch.sum(top, dim=1)  # (m,1)\n",
    "            S = torch.div(top, bottom.reshape(batch_size, stations,1))\n",
    "            I = torch.eye(self.stations, self.stations).to(device)\n",
    "            A = S + I\n",
    "            sumA = torch.sum(A, dim=1)\n",
    "            A_ = torch.div(A, sumA.reshape(batch_size, stations,1))\n",
    "            Gcn = torch.matmul(input_, self.Wgcn)\n",
    "            Gcn = torch.matmul(A_, Gcn) #batch,n,hidden_size\n",
    "            Gcn1 = torch.matmul(input_, self.Wgcn1)\n",
    "            Gcn1 = torch.matmul(A_ **2, Gcn1) #batch,n,hidden_size\n",
    "            temporalM[:, index, :, :] = torch.matmul(torch.cat([Gcn, Gcn1], dim=2), self.wfusion)         \n",
    "            index = index+1\n",
    "        predictlist=[]\n",
    "        averagelist = []\n",
    "        for dint in decoder_inputs:          \n",
    "            averageT = torch.mean(temporalM, dim=2, keepdim=True)\n",
    "            averagecon = self.averagecon(averageT.permute(0, 3, 1, 2).to(device)) #batch,hiddensize,1,1\n",
    "            predict_avg = torch.matmul(averagecon.reshape(batch_size, self.hidden_size), self.avgwout)\n",
    "            averagelist.append(predict_avg)\n",
    "            con1 = self.con1(temporalM.permute(0, 3, 1, 2).to(device)) #batch,hiddensize,12,35\n",
    "            con2 = self.con2(con1.to(device)).permute(0, 2, 3, 1) #batch,hiddensize,1,1\n",
    "            short_input = temporalM[:, -1, :, :].permute(0, 2, 1)\n",
    "            cshort = torch.matmul(short_input.to(device), self.wshort).permute(0, 2, 1).reshape(batch_size, 1, stations, self.hidden_size) #batch, stations, hidden\n",
    "            con = torch.matmul(torch.cat([con2, cshort], dim=3), self.wc)\n",
    "            #con = cshort\n",
    "            is_teather = random.random() < teather_ratio\n",
    "            w = self.wout.permute(1, 0)\n",
    "            tout = torch.matmul(dint.reshape(batch_size, stations, 1).float(), w).reshape(batch_size, 1, stations, self.hidden_size)\n",
    "            out = (tout if is_teather else con).to(device)\n",
    "            temporalM = torch.cat([temporalM[:, 1:, :, :].to(device), out], dim=1)\n",
    "            output = con.reshape(batch_size, hidden_size, stations).permute(0, 2, 1)\n",
    "            predict = torch.matmul(output, self.wout)\n",
    "            predictlist.append(predict)\n",
    "        return predictlist,averagelist\n",
    "\n",
    "    def forward(self, x, teather_ratio):\n",
    "        encoder_inputs, labels, decoder_inputs = self.input_transform(x)\n",
    "        predicts, avgs = self.SpatialGCN(encoder_inputs, decoder_inputs, self.Ws, teather_ratio)\n",
    "        # predicts = self.Decoder(As, Af, At, encoder_inputs, h1list, h2list, h3list, self.dcell1, self.dcell2, self.dcell3)\n",
    "        return predicts, avgs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score <= self.best_score + self.delta + 0.000001:\n",
    "            self.counter += 1\n",
    "            print('EarlyStopping counter:', self.counter)\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print('Validation loss min:', self.val_loss_min)\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "def get_batch_feed_dict(k, batch_size, training_data, train_labels_data):\n",
    "    batch_train_inp = training_data[k:k + batch_size]\n",
    "    batch_label_inp = train_labels_data[k:k + batch_size]\n",
    "    feed_dict = (batch_train_inp, batch_label_inp)\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "np.random.seed(2021)\n",
    "random.seed(2021)\n",
    "torch.manual_seed(2021)\n",
    "hidden_size = 64\n",
    "feature_num = 29\n",
    "model = TGCN()\n",
    "model.to(device)\n",
    "import datetime\n",
    "\n",
    "\n",
    "print('开始时间-------', datetime.datetime.now())\n",
    "total_epoch = 500\n",
    "batch_size = 256\n",
    "lr = 0.0001\n",
    "\n",
    "traindata = np.load(\"train.npy\")\n",
    "testdata = np.load(\"test.npy\")\n",
    "pm25_train = np.load(\"pm25_train.npy\")\n",
    "pm25_test = np.load(\"pm25_test.npy\")\n",
    "\n",
    "max_value = 560\n",
    "min_value = 2\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08)\n",
    "index = [i for i in range(len(testdata))]\n",
    "random.shuffle(index)\n",
    "testdata = testdata[index]\n",
    "pm25_test = pm25_test[index]\n",
    "\n",
    "def criterion(preds, avgs, labels, para):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = 0.0\n",
    "    for ps, avg, ls in zip(preds, avgs, labels):\n",
    "        ls_avg = torch.mean(ls, dim=1, keepdim=False)\n",
    "        loss = loss + loss_fn(ps.cpu().float(), ls.cpu().float())+ para*loss_fn(avg.cpu().float(), ls_avg.cpu().float())\n",
    "    return loss\n",
    "\n",
    "num_train = 7777\n",
    "num_valid = 1193\n",
    "num_test = 1193\n",
    "\n",
    "train_data = torch.from_numpy(traindata[:num_train, :, :, :]).to(device)  # 6990, 12, 34, 26\n",
    "train_labels_data = torch.from_numpy(pm25_train[:num_train, :, :, :]).to(device)  # 6990, 6, 1, 26\n",
    "\n",
    "valid_data = torch.from_numpy(traindata[num_train:, :, :, :]).to(device)\n",
    "valid_labels_data = torch.from_numpy(pm25_train[num_train:, :, :]).to(device)\n",
    "\n",
    "test_data = torch.from_numpy(testdata[:, :, :, :]).to(device)\n",
    "test_labels_data = torch.from_numpy(pm25_test[:, :, :]).to(device)\n",
    "\n",
    "# 64,12,34,26    64,6,1,26\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "avg_train_losses = []\n",
    "avg_valid_losses = []\n",
    "\n",
    "early_stopping = EarlyStopping(patience=15, verbose=True)\n",
    "\n",
    "teather_ratio=0.5\n",
    "te = 0.0\n",
    "k=0.5\n",
    "para = 0.3\n",
    "for i in range(total_epoch):\n",
    "    print('----------epoch {}-----------{}'.format(i, datetime.datetime.now()))\n",
    "    i += 1\n",
    "    teather_ratio = teather_ratio * (0.9 ** (i // 30))\n",
    "    for j in range(0, num_train, batch_size):\n",
    "        x = get_batch_feed_dict(j, batch_size, train_data, train_labels_data)\n",
    "        preds,avgs, labels = model(x,teather_ratio)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(preds, avgs, labels, para)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    for j in range(0, num_valid, batch_size):\n",
    "        x = get_batch_feed_dict(j, batch_size, valid_data, valid_labels_data)\n",
    "        preds,avgs, labels = model(x,te)\n",
    "        loss = criterion(preds,avgs, labels, para)\n",
    "        valid_losses.append(loss.item())\n",
    "    train_loss = np.average(train_losses)\n",
    "    valid_loss = np.average(valid_losses)\n",
    "    avg_train_losses.append(train_loss)\n",
    "    avg_valid_losses.append(valid_loss)\n",
    "\n",
    "    early_stopping(valid_loss, model)\n",
    "    print_msg = (' train_loss:' + str(train_loss) + ' valid_loss:' + str(valid_loss))\n",
    "    print(print_msg)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "\n",
    "# test\n",
    "def MAEloss(preds, labels):\n",
    "    loss_fn = nn.L1Loss()\n",
    "    loss = 0.0\n",
    "    for ps, ls in zip(preds, labels):\n",
    "        loss = loss + loss_fn(ps.cpu().float() * (max_value - min_value) + min_value,\n",
    "                                  ls.cpu().float() * (max_value - min_value) + min_value)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def RMSEloss(preds, labels):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = 0.0\n",
    "    for ps, ls in zip(preds, labels):\n",
    "        loss = loss + loss_fn(ps.cpu().float() * (max_value - min_value) + min_value,\n",
    "                                  ls.cpu().float() * (max_value - min_value) + min_value)\n",
    "    return loss\n",
    "\n",
    "\n",
    "rmses = []\n",
    "maes = []\n",
    "test_loss = []\n",
    "test_loss1 = []\n",
    "for k in range(0, num_test, batch_size):\n",
    "    x = get_batch_feed_dict(k, batch_size, test_data, test_labels_data)\n",
    "    preds,avgs, labels= model(x,te)\n",
    "    loss = RMSEloss(preds, labels) / 6\n",
    "    loss1 = MAEloss(preds, labels) / 6\n",
    "    test_loss.append(loss.item())\n",
    "    test_loss1.append(loss1.item())\n",
    "\n",
    "print('===============METRIC===============')\n",
    "MAE = np.average(test_loss1)\n",
    "mseLoss = math.sqrt(np.average(test_loss))\n",
    "print('MAE = {:.6f}'.format(MAE))\n",
    "print('RMSE = {:.6f}'.format(mseLoss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
